# Code Context Environment Variables Example
# 
# Copy this file to ~/.context/.env as a global setting, modify the values as needed
#
# Usage: cp env.example ~/.context/.env
#
# Note: This file does not work if you put it in your codebase directory (because it may conflict with the environment variables of your codebase project)

# =============================================================================
# Embedding Provider Configuration
# =============================================================================

# Embedding provider: openai, voyage, gemini, ollama (lowercase only)
EMBEDDING_PROVIDER=openai

# Embedding model (provider-specific)
EMBEDDING_MODEL=text-embedding-3-small

# Embedding dimension (optional, overrides auto-detection)
# Use this to manually specify the embedding dimension instead of auto-detecting.
# This is useful when using custom models or when you want to avoid the initial API call for dimension detection.
# Common dimensions: text-embedding-3-small=1536, text-embedding-3-large=3072, text-embedding-ada-002=1536
# EMBEDDING_DIMENSION=1536

# Embedding batch size for processing (default: 100)
# You can customize it according to the throughput of your embedding model. Generally, larger batch size means less indexing time.
EMBEDDING_BATCH_SIZE=100

# =============================================================================
# OpenAI Configuration
# =============================================================================

# OpenAI API key
OPENAI_API_KEY=your-openai-api-key-here

# OpenAI base URL (optional, for custom endpoints)
# OPENAI_BASE_URL=https://api.openai.com/v1

# =============================================================================
# VoyageAI Configuration
# =============================================================================

# VoyageAI API key
# VOYAGEAI_API_KEY=your-voyageai-api-key-here

# =============================================================================
# Gemini Configuration
# =============================================================================

# Google Gemini API key
# GEMINI_API_KEY=your-gemini-api-key-here

# Gemini API base URL (optional, for custom endpoints)
# GEMINI_BASE_URL=https://generativelanguage.googleapis.com/v1beta

# =============================================================================
# Ollama Configuration
# =============================================================================

# Ollama model name
# OLLAMA_MODEL=

# Ollama host (default: http://localhost:11434)
# OLLAMA_HOST=http://localhost:11434

# =============================================================================
# Code Splitter Configuration
# =============================================================================

# Code splitter type: ast, langchain
SPLITTER_TYPE=ast

# =============================================================================
# Custom File Processing Configuration
# =============================================================================

# Additional file extensions to include beyond defaults (comma-separated)
# Example: .vue,.svelte,.astro,.twig,.blade.php
# CUSTOM_EXTENSIONS=.vue,.svelte,.astro

# Additional ignore patterns to exclude files/directories (comma-separated)
# Example: temp/**,*.backup,private/**,uploads/**
# CUSTOM_IGNORE_PATTERNS=temp/**,*.backup,private/**

# Whether to use hybrid search mode. If true, it will use both dense vector and BM25; if false, it will use only dense vector search.
# HYBRID_MODE=true
